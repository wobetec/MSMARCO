{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ir_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mir_datasets\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ir_datasets'"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_subset(dataset_name, sample_percentage, X, seed=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Cria um subconjunto do dataset com base na porcentagem de queries a serem amostradas e na quantidade de documentos não relevantes a serem amostrados para cada query relevante.\n",
    "\n",
    "    dataset_name: nome do dataset a ser carregado\n",
    "    sample_percentage: porcentagem de queries a serem amostradas\n",
    "    X: número de documentos não relevantes a serem amostrados para cada query\n",
    "    seed: semente para o gerador de números aleatórios\n",
    "    verbose: se True, imprime informações adicionais durante o processo\n",
    "\n",
    "    Retorna:\n",
    "    subset_queries_dict: dicionário com as queries selecionadas\n",
    "    subset_docs: dicionário com os documentos selecionados\n",
    "    subset_qrels: lista com os qrels selecionados\n",
    "    \n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    dataset = ir_datasets.load(dataset_name)\n",
    "    \n",
    "    # --- 1) Carrega queries e define subconjunto ---\n",
    "    queries_list = list(tqdm(dataset.queries_iter(), desc=\"Lendo Queries\"))\n",
    "    total_queries = len(queries_list)\n",
    "    if verbose:\n",
    "        print(f\"Total de queries: {total_queries}\")\n",
    "\n",
    "    if sample_percentage <= 0:\n",
    "        if verbose:\n",
    "            print(\"sample_percentage <= 0; retornando subconjunto vazio.\")\n",
    "        return {}, {}, []\n",
    "    elif sample_percentage >= 1:\n",
    "        # Pega todas as queries\n",
    "        subset_queries = queries_list\n",
    "        if verbose:\n",
    "            print(\"sample_percentage >= 1; usando todas as queries.\")\n",
    "    else:\n",
    "        num_to_sample = int(total_queries * sample_percentage)\n",
    "        num_to_sample = max(num_to_sample, 1)\n",
    "        if verbose:\n",
    "            print(f\"Número de queries a serem selecionadas: {num_to_sample}\")\n",
    "        subset_queries = random.sample(queries_list, num_to_sample)\n",
    "\n",
    "    subset_queries_dict = {q.query_id: q for q in subset_queries}\n",
    "    n_queries_sub = len(subset_queries_dict)\n",
    "    if n_queries_sub == 0:\n",
    "        if verbose:\n",
    "            print(\"Nenhuma query selecionada, retornando subconjunto vazio.\")\n",
    "        return {}, {}, []\n",
    "\n",
    "    # --- 2) Filtra qrels e coleta doc_ids relevantes ---\n",
    "    subset_qrels = []\n",
    "    relevant_doc_ids = set()\n",
    "    for qrel in tqdm(dataset.qrels_iter(), desc=\"Filtrando Qrels\"):\n",
    "        if qrel.query_id in subset_queries_dict:\n",
    "            subset_qrels.append(qrel)\n",
    "            relevant_doc_ids.add(qrel.doc_id)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Qrels selecionados: {len(subset_qrels)}\")\n",
    "        print(f\"Doc IDs relevantes: {len(relevant_doc_ids)}\")\n",
    "\n",
    "    # Total de documentos não relevantes que queremos\n",
    "    total_random_needed = X * n_queries_sub\n",
    "\n",
    "    # --- 3) 1 passada sobre docs_iter() usando reservoir sampling ---\n",
    "    subset_docs = {}  # Aqui vão doc_id -> doc_obj relevantes + amostrados no reservoir\n",
    "    reservoir = []    # Armazena temporariamente os documentos não relevantes escolhidos\n",
    "    num_docs_vistos_nao_rel = 0\n",
    "\n",
    "    docs_iter = dataset.docs_iter()\n",
    "    \n",
    "    for doc in tqdm(docs_iter, desc=\"Processando Docs\"):\n",
    "        d_id = doc.doc_id\n",
    "        if d_id in relevant_doc_ids:\n",
    "            # Doc relevante entra direto no subset\n",
    "            subset_docs[d_id] = doc\n",
    "        else:\n",
    "            # Doc não relevante => aplicar reservoir sampling\n",
    "            if len(reservoir) < total_random_needed:\n",
    "                # Ainda não preenchemos o reservatório\n",
    "                reservoir.append(doc)\n",
    "            else:\n",
    "                # Já temos o reservatório cheio, decide se substitui algum\n",
    "                # Importante para evitar viés de amostragem\n",
    "                j = random.randint(0, num_docs_vistos_nao_rel)\n",
    "                if j < total_random_needed:\n",
    "                    reservoir[j] = doc\n",
    "            num_docs_vistos_nao_rel += 1\n",
    "\n",
    "    # Por fim, adiciona os docs não relevantes ao subset\n",
    "    for doc in reservoir:\n",
    "        subset_docs[doc.doc_id] = doc \n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Total de Queries no subset: {n_queries_sub}\")\n",
    "        print(f\"Total de Documentos relevantes (dos qrels): {len(relevant_doc_ids)}\")\n",
    "        print(f\"Total de Documentos amostrados (não relevantes): {len(reservoir)}\")\n",
    "        print(f\"Total de Documentos no subset: {len(subset_docs)}\")\n",
    "        print(f\"Total de Qrels (original, sem alterações): {len(subset_qrels)}\")\n",
    "\n",
    "    return subset_queries_dict, subset_docs, subset_qrels\n",
    "\n",
    "\n",
    "def create_and_save_dataset(dataset_name, sample_percentage, X, output_file, seed=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Cria o subset do dataset usando a função create_subset e salva os dados em um arquivo local.\n",
    "    \"\"\"\n",
    "    subset_queries_dict, subset_docs, subset_qrels = create_subset(\n",
    "        dataset_name, sample_percentage, X, seed, verbose\n",
    "    )\n",
    "    \n",
    "    data_to_save = {\n",
    "        'queries': subset_queries_dict,\n",
    "        'docs': subset_docs,\n",
    "        'qrels': subset_qrels\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Dataset salvo com sucesso em '{output_file}'.\")\n",
    "\n",
    "def load_dataset(input_file):\n",
    "    \"\"\"\n",
    "    Lê e retorna os dados do dataset salvos localmente a partir do arquivo input_file.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'rb') as f:\n",
    "        data_loaded = pickle.load(f)\n",
    "    return data_loaded\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == '__main__':\n",
    "    dataset_name = \"msmarco-passage-v2/train\"\n",
    "    sample_percentage = 0.05\n",
    "    X = 10  \n",
    "    output_file = \"../data/subset_msmarco_train.pkl\"\n",
    "    seed = 42\n",
    "    verbose = True\n",
    "\n",
    "    # Cria e salva o dataset\n",
    "    create_and_save_dataset(dataset_name, sample_percentage, X, output_file, seed, verbose)\n",
    "\n",
    "    # Lê o dataset salvo\n",
    "    dataset_loaded = load_dataset(output_file)\n",
    "    print(\"\\nDataset carregado:\")\n",
    "    print(\"Queries:\", len(dataset_loaded['queries']))\n",
    "    print(\"Docs:\", len(dataset_loaded['docs']))\n",
    "    print(\"Qrels:\", len(dataset_loaded['qrels']))\n",
    "    \n",
    "    # exemplo de uso do dataset\n",
    "    for query in dataset_loaded['queries'].values():\n",
    "        print(query)\n",
    "        break\n",
    "\n",
    "    for doc in dataset_loaded['docs'].values():\n",
    "        print(doc)\n",
    "        break\n",
    "\n",
    "    for qrel in dataset_loaded['qrels']:\n",
    "        print(qrel)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] If you have a local copy of https://msmarco.z22.web.core.windows.net/msmarcoranking/passv2_train_queries.tsv, you can symlink it here to avoid downloading it again: /Users/bruno.lunardon/.ir_datasets/downloads/1835f44e6792c51aa98eed722a8dcc11\n",
      "[INFO] [starting] https://msmarco.z22.web.core.windows.net/msmarcoranking/passv2_train_queries.tsv\n",
      "[INFO] [finished] https://msmarco.z22.web.core.windows.net/msmarcoranking/passv2_train_queries.tsv: [00:10] [11.6MB] [1.09MB/s]\n",
      "[INFO] If you have a local copy of https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco_v2_passage.tar, you can symlink it here to avoid downloading it again: /Users/bruno.lunardon/.ir_datasets/downloads/05946bac48a8ffee62e160213eab3fda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenericQuery(query_id='121352', text='define extreme')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [starting] https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco_v2_passage.tar\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"1529511936-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"1556168704-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"7178223616-\" [2 attempts left]\n",
      "[INFO] download error: (\"Connection broken: ConnectionResetError(54, 'Connection reset by peer')\", ConnectionResetError(54, 'Connection reset by peer')). Retrying range \"11875385344-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"19633356800-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"19955253248-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"20052754432-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"20145766400-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"20242038784-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"20385267712-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"20481015808-\" [2 attempts left]\n",
      "[INFO] download error: HTTPSConnectionPool(host='msmarco.z22.web.core.windows.net', port=443): Read timed out.. Retrying range \"20612218880-\" [2 attempts left]\n",
      "[INFO] [finished] https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco_v2_passage.tar: [3:29:43] [21.8GB] [1.73MB/s]\n",
      "                                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MsMarcoV2Passage(doc_id='msmarco_passage_00_0', text='0-60 Times - 0-60 | 0 to 60 Times & 1/4 Mile Times | Zero to 60 Car Reviews.', spans=((0, 75),), msmarco_document_id='msmarco_doc_00_0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_name = \"msmarco-passage-v2/train\"\n",
    "dataset = ir_datasets.load(dataset_name)\n",
    "\n",
    "for query in dataset.queries_iter():\n",
    "    print(query)\n",
    "    break\n",
    "\n",
    "for doc in dataset.docs_iter():\n",
    "    print(doc)\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
