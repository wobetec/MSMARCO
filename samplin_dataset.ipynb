{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_subset_efficient(dataset, sample_percentage, X, seed=None, verbose=False, all_doc_ids=None):\n",
    "    \"\"\"\n",
    "    Cria um subconjunto eficiente do dataset, evitando iteração completa.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    " \n",
    "    # --- 1) Amostra de Queries ---\n",
    "    query_list = list(dataset.queries_iter())  # Coletamos todas as queries\n",
    "    total_queries = len(query_list)\n",
    "    \n",
    "    if sample_percentage >= 1:\n",
    "        sampled_queries = query_list\n",
    "    else:\n",
    "        num_to_sample = max(1, int(total_queries * sample_percentage))\n",
    "        sampled_queries = random.sample(query_list, num_to_sample)\n",
    "\n",
    "    sampled_query_ids = {q.query_id for q in sampled_queries}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total queries: {total_queries}, Queries selecionadas: {len(sampled_queries)}\")\n",
    "\n",
    "    # --- 2) Coletar documentos relevantes pelos qrels ---\n",
    "    qrels_dict = {}  # Mapeia query_id para lista de documentos relevantes\n",
    "    relevant_doc_ids = set()\n",
    "\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel.query_id in sampled_query_ids:\n",
    "            if qrel.query_id not in qrels_dict:\n",
    "                qrels_dict[qrel.query_id] = []\n",
    "            qrels_dict[qrel.query_id].append(qrel)\n",
    "            relevant_doc_ids.add(qrel.doc_id)\n",
    "\n",
    "    # --- 3) Encontrar documentos não relevantes sem iterar tudo ---\n",
    "    if all_doc_ids is None:\n",
    "        all_doc_ids = {doc.doc_id for doc in dataset.docs_iter()}  # Obtém todos os IDs disponíveis\n",
    "    non_relevant_doc_ids = list(all_doc_ids - relevant_doc_ids)  # IDs de docs não relevantes\n",
    "\n",
    "    # Amostragem de documentos não relevantes\n",
    "    sampled_non_relevant_docs = random.sample(non_relevant_doc_ids, min(len(non_relevant_doc_ids), X * len(sampled_queries)))\n",
    "\n",
    "    # --- 4) Recuperar apenas os documentos necessários ---\n",
    "    doc_store = dataset.docs_store()\n",
    "    \n",
    "    relevant_docs = doc_store.get_many(relevant_doc_ids)\n",
    "    non_relevant_docs = doc_store.get_many(sampled_non_relevant_docs)\n",
    "\n",
    "    subset_docs = {}\n",
    "    subset_docs.update(relevant_docs)  # Adiciona os documentos relevantes\n",
    "    subset_docs.update(non_relevant_docs)  # Adiciona os não relevantes\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Docs relevantes: {len(relevant_docs)}, Docs não relevantes: {len(non_relevant_docs)}, Total Docs: {len(subset_docs)}\")\n",
    "\n",
    "    return {q.query_id: q for q in sampled_queries}, subset_docs, [qrel for qrels in qrels_dict.values() for qrel in qrels]\n",
    "\n",
    "\n",
    "def create_and_save_dataset_efficient(dataset_name, sample_percentage, X, output_file, seed=None, verbose=False, all_doc_ids=None):\n",
    "    subset_queries_dict, subset_docs, subset_qrels = create_subset_efficient(\n",
    "        dataset_name, sample_percentage, X, seed, verbose, all_doc_ids\n",
    "    )\n",
    "    \n",
    "    data_to_save = {\n",
    "        'queries': subset_queries_dict,\n",
    "        'docs': subset_docs,\n",
    "        'qrels': subset_qrels\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Dataset salvo em '{output_file}'.\")\n",
    "\n",
    "def load_dataset(input_file):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    dataset_name = \"msmarco-passage-v2/train\"\n",
    "    sample_percentage = 0.01\n",
    "    X = 10\n",
    "    output_file = \"subset_msmarco_train\"\n",
    "    seed = 42\n",
    "    verbose = True\n",
    "    dataset = ir_datasets.load(dataset_name)\n",
    "    \n",
    "    all_doc_ids = {doc.doc_id for doc in tqdm(dataset.docs_iter()[:10_000_000], desc=\"Collecting doc IDs\")}  # Obtém todos os IDs disponíveis\n",
    "    print(len(all_doc_ids))\n",
    "\n",
    "    print(\"Generating subsets...\")\n",
    "    for pct in [0.01, 0.05, 0.1]:\n",
    "        for X in [9, 99, 999]:\n",
    "            create_and_save_dataset_efficient(dataset, pct, X, output_file+\"_\"+str(pct)+\"_\"+str(X)+\".pkl\", seed, verbose, all_doc_ids)\n",
    "\n",
    "    for pct in [0.01, 0.05, 0.1]:\n",
    "        for X in [9, 99, 999]:\n",
    "            output_file = \"subset_msmarco_train_\"+str(pct)+\"_\"+str(X)+\".pkl\"\n",
    "            dataset_loaded = load_dataset(output_file)\n",
    "            print(\"\\nDataset carregado:\")\n",
    "            print(\"Queries:\", len(dataset_loaded['queries']))\n",
    "            print(\"Docs:\", len(dataset_loaded['docs']))\n",
    "            print(\"Qrels:\", len(dataset_loaded['qrels']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
